{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   val1  val2  val3  class\n",
      "0    74    85   123      1\n",
      "1    73    84   122      1\n",
      "2    72    83   121      1\n",
      "3    70    81   119      1\n",
      "4    70    81   119      1\n",
      "Counter({0: 194198, 1: 50859})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split,KFold,cross_validate\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_score,roc_auc_score,accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import mlrose_hiive as mlrose\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import time\n",
    "\n",
    "# #extract the data\n",
    "raw_df = pd.read_csv('../Skin_NonSkin.txt', delimiter='\t',names=[\"val1\", \"val2\", \"val3\", \"class\"])\n",
    "cleaned_df = raw_df.copy()\n",
    "\n",
    "\n",
    "cleaned_df['class']=np.array([1 if x==1 else 0 for x in cleaned_df['class']])\n",
    "print(cleaned_df.head(5))\n",
    "#summarize data distribution\n",
    "y=np.array(cleaned_df['class'])\n",
    "X=np.array(cleaned_df.iloc[:,:-1])\n",
    "print(Counter(y))\n",
    "\n",
    "#rescale data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##cross validation to find out the best hidden_layer_size ###\n",
    "\n",
    "t=5\n",
    "cv = KFold(n_splits=t,shuffle=True)\n",
    "\n",
    "RHC_AUC_test_score_bias_list=[]\n",
    "RHC_AUC_test_score_variance_list=[]\n",
    "SA_AUC_test_score_bias_list=[]\n",
    "SA_AUC_test_score_variance_list=[]\n",
    "GA_AUC_test_score_bias_list = []\n",
    "GA_AUC_test_score_variance_list = []\n",
    "RHC_AUC_train_score_bias_list=[]\n",
    "SA_AUC_train_score_bias_list=[]\n",
    "GA_AUC_train_score_bias_list=[]\n",
    "\n",
    "\n",
    "hidden_layer_sizes = [(5,j,k) for j in range(6,8) for k in range(5,7)]\n",
    "\n",
    "# parameters=(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None, **kwargs)[source]\n",
    "for hidden_layer_size in hidden_layer_sizes:\n",
    "    RHC_NN_model = mlrose.NeuralNetwork(hidden_nodes=hidden_layer_size, activation='tanh', algorithm='random_hill_climb', \n",
    "                                    max_iters=50, bias=True, is_classifier=True, \n",
    "                                    learning_rate=0.1, early_stopping=True, \n",
    "                                    clip_max=1e+10, restarts=10, max_attempts=10, random_state=3, curve=False)\n",
    "    SA_NN_model = mlrose.NeuralNetwork(hidden_nodes=hidden_layer_size, activation='tanh', algorithm='simulated_annealing', \n",
    "                                    max_iters=50, bias=True, is_classifier=True, \n",
    "                                    learning_rate=0.1, early_stopping=True, \n",
    "                                    clip_max=1e+10, schedule= mlrose.GeomDecay(), max_attempts=10, random_state=3, curve=False)\n",
    "    GA_NN_model = mlrose.NeuralNetwork(hidden_nodes=hidden_layer_size, activation='tanh', algorithm='genetic_alg', \n",
    "                                    max_iters=50, bias=True, is_classifier=True, \n",
    "                                    learning_rate=0.1, early_stopping=True, \n",
    "                                    clip_max=1e+10, pop_size=200, mutation_prob=0.1,max_attempts=10, random_state=3, curve=False)\n",
    "#     MLP = MLPClassifier(hidden_layer_sizes=hidden_layer_size,activation ='tanh',solver='sgd',max_iter=200)\n",
    "    RHC_AUC_test_score = []\n",
    "    SA_AUC_test_score = []\n",
    "    GA_AUC_test_score = []\n",
    "    RHC_AUC_train_score = []\n",
    "    SA_AUC_train_score = []\n",
    "    GA_AUC_train_score = []\n",
    "    \n",
    "\n",
    "    for train_index, test_index in cv.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "#         start_time = time.time()\n",
    "        RHC_NN_model.fit(X_train,y_train)\n",
    "        print(\"RHC train finished\")\n",
    "#         RHC_training_time_total += time.time()-start_time()\n",
    "#         start_time = time.time()\n",
    "        SA_NN_model.fit(X_train,y_train)\n",
    "        print(\"SA train finished\")\n",
    "#         SA_training_time_total += time.time()-start_time()\n",
    "#         start_time = time.time()\n",
    "        GA_NN_model.fit(X_train,y_train)\n",
    "        print(\"GA train finished\")\n",
    "#         GA_training_time_total += time.time()-start_time()\n",
    "        \n",
    "        RHC_y_train_predict = RHC_NN_model.predict(X_train)\n",
    "        RHC_y_train_predicted_probs = RHC_NN_model.predicted_probs\n",
    "        SA_y_train_predict = RHC_NN_model.predict(X_train)\n",
    "        SA_y_train_predicted_probs = RHC_NN_model.predicted_probs\n",
    "        GA_y_train_predict = GA_NN_model.predict(X_train)\n",
    "        GA_y_train_predicted_probs = GA_NN_model.predicted_probs\n",
    "        RHC_AUC_train_score.append(roc_auc_score(y_train,RHC_y_train_predicted_probs))\n",
    "        SA_AUC_train_score.append(roc_auc_score(y_train,SA_y_train_predicted_probs))\n",
    "        GA_AUC_train_score.append(roc_auc_score(y_train,GA_y_train_predicted_probs))\n",
    "        \n",
    "        RHC_y_test_predict = RHC_NN_model.predict(X_test)\n",
    "        RHC_y_test_predicted_probs = RHC_NN_model.predicted_probs\n",
    "        SA_y_test_predict = SA_NN_model.predict(X_test)\n",
    "        SA_y_test_predicted_probs = SA_NN_model.predicted_probs\n",
    "        GA_y_test_predict = GA_NN_model.predict(X_test)\n",
    "        GA_y_test_predicted_probs = GA_NN_model.predicted_probs\n",
    "        RHC_AUC_test_score.append(roc_auc_score(y_test,RHC_y_test_predicted_probs))\n",
    "        SA_AUC_test_score.append(roc_auc_score(y_test,SA_y_test_predicted_probs))\n",
    "        GA_AUC_test_score.append(roc_auc_score(y_test,GA_y_test_predicted_probs)) \n",
    "        \n",
    "   \n",
    "    RHC_AUC_train_score_bias_list.append(np.array(RHC_AUC_train_score).mean())\n",
    "    RHC_AUC_test_score_bias_list.append(np.array(RHC_AUC_test_score).mean())\n",
    "    RHC_AUC_test_score_variance_list.append(np.array(RHC_AUC_test_score).std()**2)\n",
    "    \n",
    "    SA_AUC_train_score_bias_list.append(np.array(SA_AUC_train_score).mean())\n",
    "    SA_AUC_test_score_bias_list.append(np.array(SA_AUC_test_score).mean())\n",
    "    SA_AUC_test_score_variance_list.append(np.array(SA_AUC_test_score).std()**2)\n",
    "    \n",
    "    GA_AUC_train_score_bias_list.append(np.array(GA_AUC_train_score).mean())\n",
    "    GA_AUC_test_score_bias_list.append(np.array(GA_AUC_test_score).mean())\n",
    "    GA_AUC_test_score_variance_list.append(np.array(GA_AUC_test_score).std()**2)\n",
    "    \n",
    "\n",
    "#generate tunning graph#######\n",
    "# m = len(hidden_layer_sizes)\n",
    "hidden_layer_sizes_text=[str(hidden_layer_size[0])+str(hidden_layer_size[1])+str(hidden_layer_size[2]) for hidden_layer_size in hidden_layer_sizes]\n",
    "fig_0, axs = plt.subplots(1, 2, figsize=(10, 5), sharey=False)\n",
    "axs[0].plot(hidden_layer_sizes_text,RHC_AUC_test_score_bias_list,\"r^\",linestyle = \"--\",label='RHC AUC_Bias')\n",
    "axs[0].plot(hidden_layer_sizes_text,SA_AUC_test_score_bias_list,\"b^\",linestyle = \"--\",label='SA AUC_Bias')\n",
    "axs[0].plot(hidden_layer_sizes_text,GA_AUC_test_score_bias_list,\"g^\",linestyle = \"--\",label='GA AUC_Bias')\n",
    "\n",
    "axs[1].plot(hidden_layer_sizes_text,RHC_AUC_test_score_variance_list,\"r^\",linestyle = \"--\",label='RHC AUC_Variance')\n",
    "axs[1].plot(hidden_layer_sizes_text,SA_AUC_test_score_variance_list,\"b^\",linestyle = \"--\",label='SA AUC_Variance')\n",
    "axs[1].plot(hidden_layer_sizes_text,GA_AUC_test_score_variance_list,\"g^\",linestyle = \"--\",label='GA AUC_Variance')\n",
    "\n",
    "axs[0].set_xlabel(\"hidden_layer_size\")\n",
    "axs[0].set_ylabel(\"AUC_Bias\")\n",
    "axs[1].set_xlabel(\"hidden_layer_size\")\n",
    "axs[1].set_ylabel(\"AUC_Variance\")\n",
    "\n",
    "axs[0].legend()\n",
    "axs[1].legend()\n",
    "\n",
    "fig_0.suptitle(\"Three random NN model: Bias/Variance Comparison(NN tunning max_iter=50)\")\n",
    "fig_0.savefig(\"Three random NN model: Bias-Variance Comparison.png\")\n",
    "\n",
    "\n",
    "\n",
    "fig_1,axs = plt.subplots(1,3,figsize=(15,5),sharey=False)\n",
    "axs[0].plot(hidden_layer_sizes_text,RHC_AUC_train_score_bias_list,\"r^\",linestyle = \"--\",label='training data')\n",
    "axs[0].plot(hidden_layer_sizes_text,RHC_AUC_test_score_bias_list,\"b^\",linestyle = \"--\",label='test data')\n",
    "\n",
    "\n",
    "axs[1].plot(hidden_layer_sizes_text,SA_AUC_train_score_bias_list,\"r^\",linestyle = \"--\",label='training data')\n",
    "axs[1].plot(hidden_layer_sizes_text,SA_AUC_test_score_bias_list,\"b^\",linestyle = \"--\",label='test data')\n",
    "\n",
    "axs[2].plot(hidden_layer_sizes_text,GA_AUC_train_score_bias_list,\"r^\",linestyle = \"--\",label='training data')\n",
    "axs[2].plot(hidden_layer_sizes_text,GA_AUC_test_score_bias_list,\"b^\",linestyle = \"--\",label='test data')\n",
    "axs[0].set_xlabel(\"hidden_layer_size\")\n",
    "axs[0].set_ylabel(\"RHC_AUC_score\")\n",
    "axs[1].set_xlabel(\"hidden_layer_size\")\n",
    "axs[1].set_ylabel(\"SA_AUC_score\")\n",
    "axs[2].set_xlabel(\"hidden_layer_size\")\n",
    "axs[2].set_ylabel(\"GA_AUC_score\")\n",
    "axs[0].legend()\n",
    "axs[1].legend()\n",
    "axs[2].legend()\n",
    "fig_1.suptitle(\"Three NN models' AUC_score(NN tuning max_iter=50)\")\n",
    "fig_1.savefig(\"Three NN models' AUC_score(NN tuning max_iter=50).png\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###compare the learning curve with MLP with different training size#########\n",
    "\n",
    "MLP_Accuracy_train_score=[]\n",
    "MLP_Accuracy_test_score=[]\n",
    "RHC_Accuracy_train_score=[]\n",
    "RHC_Accuracy_test_score =[]\n",
    "SA_Accuracy_train_score=[]\n",
    "SA_Accuracy_test_score=[]\n",
    "GA_Accuracy_train_score=[]\n",
    "GA_Accuracy_test_score=[]\n",
    "MLP_Precision_test_score=[]\n",
    "MLP_Precision_train_score=[]\n",
    "RHC_Precision_test_score=[]\n",
    "RHC_Precision_train_score=[]\n",
    "SA_Precision_test_score=[]\n",
    "SA_Precision_train_score=[]\n",
    "GA_Precision_test_score=[]\n",
    "GA_Precision_train_score=[]\n",
    "MLP_training_time = []\n",
    "RHC_training_time = []\n",
    "SA_training_time = []\n",
    "GA_training_time = []\n",
    "MLP_training_time =[]\n",
    "for k in range(4,11):\n",
    "    MLP = MLPClassifier(hidden_layer_sizes=(5,7,6),activation ='tanh',solver='sgd',max_iter=50)\n",
    "    \n",
    "    RHC_NN_model = mlrose.NeuralNetwork(hidden_nodes=(5,7,5), activation='tanh', algorithm='random_hill_climb',\n",
    "                                    max_iters=50, bias=True, is_classifier=True,\n",
    "                                    learning_rate=0.1, early_stopping=True,\n",
    "                                    clip_max=1e+10, restarts=10, max_attempts=10, random_state=3, curve=False)\n",
    "    SA_NN_model = mlrose.NeuralNetwork(hidden_nodes=(5,7,5), activation='tanh', algorithm='simulated_annealing',\n",
    "                                    max_iters=50, bias=True, is_classifier=True,\n",
    "                                    learning_rate=0.1, early_stopping=True,\n",
    "                                    clip_max=1e+10, schedule= mlrose.GeomDecay(), max_attempts=10, random_state=3, curve=False)\n",
    "    GA_NN_model = mlrose.NeuralNetwork(hidden_nodes=(5,7,6), activation='tanh', algorithm='genetic_alg',\n",
    "                                    max_iters=50, bias=True, is_classifier=True,\n",
    "                                    learning_rate=0.1, early_stopping=True,\n",
    "                                    clip_max=1e+10, pop_size=200, mutation_prob=0.1,max_attempts=10, random_state=3, curve=False)\n",
    "    print(k)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1/k, random_state = 3)\n",
    "    \n",
    "    start_time=time.time()\n",
    "    MLP.fit(X_train,y_train)\n",
    "    MLP_training_time.append(time.time()-start_time)\n",
    "    print(\"MLP train finished\")\n",
    "    MLP_y_train_predict = MLP.predict(X_train)\n",
    "    MLP_y_test_predict = MLP.predict(X_test)\n",
    "    MLP_Accuracy_train_score.append(accuracy_score(y_train,MLP_y_train_predict))\n",
    "    MLP_Accuracy_test_score.append(accuracy_score(y_test,MLP_y_test_predict))\n",
    "    MLP_Precision_test_score.append(precision_score(y_test,MLP_y_test_predict))\n",
    "    MLP_Precision_train_score.append(precision_score(y_train,MLP_y_train_predict))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    RHC_NN_model.fit(X_train,y_train)\n",
    "    RHC_training_time.append(time.time()-start_time)\n",
    "    print(\"RHC train finished\")\n",
    "    RHC_y_train_predict = RHC_NN_model.predict(X_train)\n",
    "    RHC_y_test_predict = RHC_NN_model.predict(X_test)\n",
    "    RHC_Accuracy_train_score.append(accuracy_score(y_train,RHC_y_train_predict))\n",
    "    RHC_Accuracy_test_score.append(accuracy_score(y_test,RHC_y_test_predict))\n",
    "    RHC_Precision_test_score.append(precision_score(y_test,RHC_y_test_predict))\n",
    "    RHC_Precision_train_score.append(precision_score(y_train,RHC_y_train_predict))\n",
    "   \n",
    "   \n",
    "    start_time = time.time()\n",
    "    SA_NN_model.fit(X_train,y_train)\n",
    "    SA_training_time.append(time.time()-start_time)\n",
    "    print(\"SA train finished\")\n",
    "    SA_y_train_predict = SA_NN_model.predict(X_train)\n",
    "    SA_y_test_predict = SA_NN_model.predict(X_test)\n",
    "    SA_Accuracy_train_score.append(accuracy_score(y_train,SA_y_train_predict))\n",
    "    SA_Accuracy_test_score.append(accuracy_score(y_test,SA_y_test_predict))\n",
    "    SA_Precision_test_score.append(precision_score(y_test,SA_y_test_predict))\n",
    "    SA_Precision_train_score.append(precision_score(y_train,SA_y_train_predict))\n",
    "   \n",
    "   \n",
    "   \n",
    "    start_time = time.time()\n",
    "    GA_NN_model.fit(X_train,y_train)\n",
    "    GA_training_time.append(time.time()-start_time)\n",
    "    print(\"GA train finished\")\n",
    "    GA_y_train_predict = GA_NN_model.predict(X_train)\n",
    "    GA_y_test_predict = GA_NN_model.predict(X_test)\n",
    "    GA_Accuracy_train_score.append(accuracy_score(y_train,GA_y_train_predict))\n",
    "    GA_Accuracy_test_score.append(accuracy_score(y_test,GA_y_test_predict))\n",
    "    GA_Precision_test_score.append(precision_score(y_test,GA_y_test_predict))\n",
    "    GA_Precision_train_score.append(precision_score(y_train,GA_y_train_predict))\n",
    "   \n",
    "   \n",
    "\n",
    "##genearte learning curves with traning size#######\n",
    "\n",
    "K= range(4,11)\n",
    "fig_0, axs = plt.subplots(1, 1, figsize=(5, 5), sharey=False)\n",
    "axs.plot(K,RHC_training_time,\"bo\",linestyle = \"--\",label=\"RHC\")\n",
    "axs.plot(K,SA_training_time,\"y^\",linestyle = \"--\",label=\"SA\")\n",
    "axs.plot(K,GA_training_time,\"go\",linestyle = \"--\",label=\"GA\")\n",
    "axs.plot(K,MLP_training_time,'r^',linestyle = \"--\",label=\"MLP\")\n",
    "axs.set_xlabel(\"training size k\")\n",
    "axs.set_ylabel(\"training time\")\n",
    "axs.legend()\n",
    "fig_0.suptitle(\"Randomized Weight Optimization vs MLP: Training time\")\n",
    "fig_0.savefig(\"Randomized Weight Optimization vs MLP: Training time.png\")\n",
    "\n",
    "fig_1, axs = plt.subplots(1, 4, figsize=(10, 10), sharey=False)\n",
    "\n",
    "axs[0].plot(K,RHC_Accuracy_train_score,\"r^\",linestyle = \"--\",label='training data')\n",
    "axs[0].plot(K,RHC_Accuracy_test_score,\"b^\",linestyle = \"--\",label='test data')\n",
    "axs[1].plot(K,SA_Accuracy_train_score,\"r^\",linestyle = \"--\",label='training data')\n",
    "axs[1].plot(K,SA_Accuracy_test_score,\"b^\",linestyle = \"--\",label='test data')\n",
    "axs[2].plot(K,GA_Accuracy_train_score,\"r^\",linestyle = \"--\",label='training data')\n",
    "axs[2].plot(K,GA_Accuracy_test_score,\"b^\",linestyle = \"--\",label='test data')\n",
    "axs[3].plot(K,MLP_Accuracy_train_score,\"r^\",linestyle = \"--\",label='training data')\n",
    "axs[3].plot(K,MLP_Accuracy_test_score,\"b^\",linestyle = \"--\",label='test data')\n",
    "\n",
    "axs[0].set_xlabel(\"training size k\")\n",
    "axs[0].set_ylabel(\"RHC accuracy_score\")\n",
    "axs[1].set_xlabel(\"training size k\")\n",
    "axs[1].set_ylabel(\"SA accuracy_score\")\n",
    "axs[2].set_xlabel(\"training size k\")\n",
    "axs[2].set_ylabel(\"GA accuracy_score\")\n",
    "axs[3].set_xlabel(\"training size k\")\n",
    "axs[3].set_ylabel(\"MLP accuracy_score\")\n",
    "axs[0].legend()\n",
    "axs[1].legend()\n",
    "axs[2].legend()\n",
    "axs[3].legend()\n",
    "\n",
    "fig_1.suptitle(\"Randomized Optimization vs MLP: Accuracy score\")\n",
    "fig_1.savefig(\"Randomized Optimization vs MLP: Accuracy score.png\")\n",
    "\n",
    "fig_2, axs = plt.subplots(1, 4, figsize=(10, 10), sharey=False)\n",
    "\n",
    "axs[0].plot(K,RHC_Precision_train_score,\"r^\",linestyle = \"--\",label='training data')\n",
    "axs[0].plot(K,RHC_Precision_test_score,\"b^\",linestyle = \"--\",label='test data')\n",
    "axs[1].plot(K,SA_Precision_train_score,\"r^\",linestyle = \"--\",label='training data')\n",
    "axs[1].plot(K,SA_Precision_test_score,\"b^\",linestyle = \"--\",label='test data')\n",
    "axs[2].plot(K,GA_Precision_train_score,\"r^\",linestyle = \"--\",label='training data')\n",
    "axs[2].plot(K,GA_Precision_test_score,\"b^\",linestyle = \"--\",label='test data')\n",
    "axs[3].plot(K,MLP_Precision_train_score,\"r^\",linestyle = \"--\",label='training data')\n",
    "axs[3].plot(K,MLP_Precision_test_score,\"b^\",linestyle = \"--\",label='test data')\n",
    "\n",
    "axs[0].set_xlabel(\"training size k\")\n",
    "axs[0].set_ylabel(\"RHC precision_score\")\n",
    "axs[1].set_xlabel(\"training size k\")\n",
    "axs[1].set_ylabel(\"SA precision_score\")\n",
    "axs[2].set_xlabel(\"training size k\")\n",
    "axs[2].set_ylabel(\"GA precision_score\")\n",
    "axs[3].set_xlabel(\"training size k\")\n",
    "axs[3].set_ylabel(\"MLP precision_score\")\n",
    "axs[0].legend()\n",
    "axs[1].legend()\n",
    "axs[2].legend()\n",
    "axs[3].legend()\n",
    "fig_2.suptitle(\"Randomized Optimization vs MLP : Precision score\")\n",
    "fig_2.savefig(\"Randomized Optimization vs MLP: Precision score.png\")\n",
    "\n",
    "###each model's bias/variance\n",
    "RHC_Accuracy_mean = np.array(RHC_Accuracy_test_score).mean()\n",
    "RHC_Accuracy_variance = np.array(RHC_Accuracy_test_score).std()**2\n",
    "SA_Accuracy_mean = np.array(SA_Accuracy_test_score).mean()\n",
    "SA_Accuracy_variance = np.array(SA_Accuracy_test_score).std()**2\n",
    "GA_Accuracy_mean = np.array(GA_Accuracy_test_score).mean()\n",
    "GA_Accuracy_variance = np.array(GA_Accuracy_test_score).std()**2\n",
    "MLP_Accuracy_mean=np.array(MLP_Accuracy_test_score).mean()\n",
    "MLP_Accuracy_variance = np.array(MLP_Accuracy_test_score).std()**2\n",
    "\n",
    "RHC_Precision_mean = np.array(RHC_Precision_test_score).mean()\n",
    "RHC_Precision_variance = np.array(RHC_Precision_test_score).std()**2\n",
    "SA_Precision_mean = np.array(SA_Precision_test_score).mean()\n",
    "SA_Precision_variance = np.array(SA_Precision_test_score).std()**2\n",
    "GA_Precision_mean = np.array(GA_Precision_test_score).mean()\n",
    "GA_Precision_variance = np.array(GA_Precision_test_score).std()**2\n",
    "MLP_Precision_mean=np.array(MLP_Precision_test_score).mean()\n",
    "MLP_Precision_variance = np.array(MLP_Precision_test_score).std()**2\n",
    "\n",
    "fig_3, axs = plt.subplots(1, 4, figsize=(10, 10), sharey=False)\n",
    "X=[\"RHC\",\"SA\",\"GA\",\"MLP\"]\n",
    "axs[0].bar(X,[RHC_Accuracy_mean,SA_Accuracy_mean,GA_Accuracy_mean,MLP_Accuracy_mean])\n",
    "axs[1].bar(X,[RHC_Accuracy_variance,SA_Accuracy_variance,GA_Accuracy_variance,MLP_Accuracy_variance])\n",
    "axs[2].bar(X,[RHC_Precision_mean,SA_Precision_mean,GA_Precision_mean,MLP_Precision_mean])\n",
    "axs[3].bar(X,[RHC_Precision_variance,SA_Precision_variance,GA_Precision_variance,MLP_Precision_variance])\n",
    "axs[0].set_ylabel(\"Accuracy Bias\")\n",
    "axs[1].set_ylabel(\"Accuracy Variance\")\n",
    "axs[2].set_ylabel(\"Precision Bias\")\n",
    "axs[3].set_ylabel(\"Precision Variance\")\n",
    "fig_3.suptitle(\"Accuracy and Precision Bias/Variance\")\n",
    "fig_3.savefig(\"Accuracy and Precision Bias-Variance.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
